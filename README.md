# Dual-Transformer Imageâ†’3D for Fruit Plants

Reconstruct botanically faithful **3D plant geometry** from multi-view RGB (Â±Depth). The system couples a **DINO ViT** backbone, a **Volumetric Transformer** for 3D reasoning, **Geometry-Grounded Refinement**, and a **Differentiable Renderer**. Each stage produces **viewable artifacts** (images, videos, meshes) for quick validation in **Google Colab** or **Kaggle**.

> **Highlights**
>
> * Self-supervised DINO features â†’ robust view-invariant tokens
> * 2Dâ†’3D multi-view **lifting** into a (sparse) voxel grid
> * **Volumetric Transformer** with windowed 3D attention + ray-slice attention
> * **Refinement** via cross-attention (2Dâ†”3D) + differential-geometry & botanical priors
> * End-to-end **rendering supervision** (no 3D GT required)

---

## ğŸ“ Repository Structure

```
plant3d/
â”œâ”€ notebooks/
â”‚  â”œâ”€ 00_quickstart_colab.ipynb
â”‚  â”œâ”€ 00_quickstart_kaggle.ipynb
â”‚  â”œâ”€ 10_capture_and_calibration.ipynb
â”‚  â”œâ”€ 20_dino_precompute_features.ipynb
â”‚  â”œâ”€ 30_lift_2d_to_3d_volume.ipynb
â”‚  â”œâ”€ 40_volumetric_transformer_train.ipynb
â”‚  â”œâ”€ 50_renderer_view_synthesis.ipynb
â”‚  â”œâ”€ 60_geometry_refinement_train.ipynb
â”‚  â””â”€ 70_evaluation_and_visualization.ipynb
â”œâ”€ src/
â”‚  â”œâ”€ config/
â”‚  â”‚  â”œâ”€ base.yaml
â”‚  â”‚  â”œâ”€ colab.yaml
â”‚  â”‚  â””â”€ kaggle.yaml
â”‚  â”œâ”€ dataio/
â”‚  â”‚  â”œâ”€ datasets.py          # MV dataset, masks, depth
â”‚  â”‚  â”œâ”€ camera.py            # K/E structs, pose I/O, helpers
â”‚  â”‚  â””â”€ loaders.py           # dataloaders, caching
â”‚  â”œâ”€ features/
â”‚  â”‚  â”œâ”€ dino_backbone.py     # DINO student/teacher wrappers
â”‚  â”‚  â””â”€ dino_utils.py        # multi-crop, EMA, centering
â”‚  â”œâ”€ geometry/
â”‚  â”‚  â”œâ”€ lift.py              # 2Dâ†’3D projection, voxel agg
â”‚  â”‚  â”œâ”€ grids.py             # dense/sparse octree grids
â”‚  â”‚  â””â”€ priors.py            # smoothness/curvature/connectivity
â”‚  â”œâ”€ models/
â”‚  â”‚  â”œâ”€ volumetric_transformer/
â”‚  â”‚  â”‚  â”œâ”€ blocks.py         # windowed 3D attn, ray-slice attn
â”‚  â”‚  â”‚  â”œâ”€ xscale.py         # cross-scale fusion pyramid
â”‚  â”‚  â”‚  â””â”€ heads.py          # Ïƒ (occupancy), c (color)
â”‚  â”‚  â”œâ”€ refinement/
â”‚  â”‚  â”‚  â”œâ”€ cross_grounding.py # 2Dâ†”3D cross-attention
â”‚  â”‚  â”‚  â””â”€ losses.py          # geometry-aware losses
â”‚  â”‚  â””â”€ renderer/
â”‚  â”‚     â”œâ”€ rays.py           # sampling, stratified along t
â”‚  â”‚     â””â”€ render.py         # Î±-compositing, color/depth/opacity
â”‚  â”œâ”€ train/
â”‚  â”‚  â”œâ”€ trainer_vol.py       # train volumetric transformer
â”‚  â”‚  â”œâ”€ trainer_refine.py    # train refinement + priors
â”‚  â”‚  â””â”€ optim.py             # schedulers, mixed precision, EMA
â”‚  â”œâ”€ eval/
â”‚  â”‚  â”œâ”€ metrics_3d.py        # IoU, Chamfer-L2, F-score
â”‚  â”‚  â”œâ”€ metrics_img.py       # PSNR, SSIM
â”‚  â”‚  â””â”€ mesh.py              # marching cubes, exports (PLY/OBJ)
â”‚  â””â”€ viz/
â”‚     â”œâ”€ gallery.py           # grids, side-by-side, GIF/MP4
â”‚     â””â”€ tensorboard.py
â”œâ”€ outputs/
â”‚  â”œâ”€ stage_10_capture/       # rectified images, K/E, masks
â”‚  â”œâ”€ stage_20_dino/          # feature maps (H/8Ã—W/8Ã—d)
â”‚  â”œâ”€ stage_30_volume/        # voxel features (npz), previews
â”‚  â”œâ”€ stage_40_vol_train/     # ckpts, TB logs, novel views
â”‚  â”œâ”€ stage_50_render/        # rendered novel views, depth
â”‚  â”œâ”€ stage_60_refine/        # refined volumes, meshes
â”‚  â””â”€ stage_70_eval/          # metrics.csv, plots
â”œâ”€ scripts/
â”‚  â”œâ”€ prepare_colab.sh
â”‚  â”œâ”€ prepare_kaggle.sh
â”‚  â”œâ”€ export_mesh.py
â”‚  â””â”€ demo_render.py
â”œâ”€ requirements.txt
â”œâ”€ pyproject.toml             # or setup.cfg / setup.py
â”œâ”€ README.md                  # (this file)
â””â”€ LICENSE

```

---

## ğŸš€ Quickstart

### Option A â€” Google Colab

1. Open `notebooks/00_quickstart_colab.ipynb`.
2. Run the setup cell (installs `requirements.txt`, optional Drive mount).
3. Set `DATA_ROOT` and `RUN_NAME` in the config cell.
4. Execute stages sequentially or jump to the desired notebook.

### Option B â€” Kaggle

1. In a Kaggle Notebook:

   ```bash
   !git clone https://github.com/<you>/plant3d.git
   %cd plant3d
   !pip -q install -r requirements.txt
   ```
2. Open `notebooks/00_quickstart_kaggle.ipynb`, set `DATA_ROOT=/kaggle/input/<dataset>`.

---

## ğŸ”§ Installation (local)

```bash
git clone https://github.com/<you>/plant3d.git
cd plant3d
python -m venv .venv && source .venv/bin/activate   # (Windows: .venv\Scripts\activate)
pip install -r requirements.txt
```

---

## âš™ï¸ Configuration

Edit one of:

* `src/config/base.yaml` â€“ defaults
* `src/config/colab.yaml` â€“ paths & small GPU settings
* `src/config/kaggle.yaml` â€“ Kaggle paths

**Example (`src/config/base.yaml`):**

```yaml
data:
  root: /path/to/DATA_ROOT
  scene_list: [sample_plant_001]
grid:
  type: sparse_octree
  base_res: 64
  max_res: 128
dino:
  pretrained: "facebook/dino-vitb16"
  freeze: true
train:
  epochs: 50
  batch_size: 1
  lr: 2.0e-4
renderer:
  n_samples: 64
loss:
  lambda_sil: 1.0
  lambda_depth: 0.2
  lambda_geom: 0.5
  lambda_dino: 0.1
```

---

## ğŸ§± Stages & Outputs (what youâ€™ll see)

1. **Capture & Calibration** â†’ rectified images, verified **K/E**, QC mosaics
2. **DINO Features** â†’ dense maps, attention visualizations
3. **2Dâ†’3D Lift** â†’ (sparse) voxel feature volumes, slice previews
4. **Volumetric Transformer** â†’ occupancy/color fields, novel-view renders
5. **Renderer** â†’ predicted color/depth/opacity, loss curves
6. **Refinement** â†’ improved thin structures, **meshes (PLY/OBJ)**
7. **Evaluation** â†’ IoU, Chamfer-L2, F-score, PSNR/SSIM, CSV reports

Artifacts are saved under `outputs/stage_*/â€¦` (PNGs, GIF/MP4 turntables, OBJ/PLY meshes, CSV metrics).

---

## ğŸ§ª Minimal Data Schema

```
DATA_ROOT/
â””â”€ sample_plant_001/
   â”œâ”€ rgb/*.png
   â”œâ”€ mask/*.png
   â”œâ”€ depth/*.png           # optional
   â”œâ”€ intrinsics.json       # {fx, fy, cx, cy}
   â””â”€ poses/*.json          # {R: 3x3, t: 3x1} per view (worldâ†’cam)
```

---

## ğŸ‹ï¸ Training Recipes

* **Small GPU**: 64Â³ base grid, windowed 3D attention (N1=2), ray-slice (N2=1), AMP on.
* **Medium**: 96Â³â€“128Â³ with sparse octree, gradient checkpointing.
* **Refinement**: freeze Ïƒ head, train cross-attention + priors, then fine-tune end-to-end.

---

## ğŸ”Œ Extending

* Swap DINO for other ViTs in `src/features/`.
* Replace voxel grid with tri-plane/hybrid (edit `geometry/grids.py`).
* Add depth sensors: enable `loss.lambda_depth` and `data.depth=true`.

---

## ğŸ“Š Evaluation

* 3D: **IoU**, **Chamfer-L2**, **F-score**
* Rendering: **PSNR/SSIM**
* Mesh export: `scripts/export_mesh.py` â†’ PLY/OBJ (Meshlab/Blender ready)

---

## ğŸ“œ License

Add your chosen license in `LICENSE`.

---

## ğŸ™‹ Support

* Open a GitHub Issue for bugs or feature requests.
* Want a generated scaffold (empty modules + starter notebooks) for **PyTorch** with **TensorBoard**/**wandb**? Ask and specify your preference.

